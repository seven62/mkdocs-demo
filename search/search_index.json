{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 RockNSM is the premier sensor platform for Network Security Monitoring (NSM) hunting and incident response (IR) operations. ROCK is the open-source security distribution that prioritizes being: Reliable Scalable Secure Above all else, ROCK exists to aid the analyst in the fight to find the adversary. Latest \u00b6 See the releases page for the latest info on ROCK 2.3. Contents \u00b6 About - project overview / background / dataflow Install - requirements / install media / installation Configure - configuring for your use case Deploy - development via Ansible playbooks Usage - basic usage overview and troubleshooting Services - component directory and management info Reference - concept / design, components / dataflow Continue to What is ROCK?","title":"Welcome"},{"location":"#welcome","text":"RockNSM is the premier sensor platform for Network Security Monitoring (NSM) hunting and incident response (IR) operations. ROCK is the open-source security distribution that prioritizes being: Reliable Scalable Secure Above all else, ROCK exists to aid the analyst in the fight to find the adversary.","title":"Welcome"},{"location":"#latest","text":"See the releases page for the latest info on ROCK 2.3.","title":"Latest"},{"location":"#contents","text":"About - project overview / background / dataflow Install - requirements / install media / installation Configure - configuring for your use case Deploy - development via Ansible playbooks Usage - basic usage overview and troubleshooting Services - component directory and management info Reference - concept / design, components / dataflow Continue to What is ROCK?","title":"Contents"},{"location":"about/backstory/","text":"Backstory \u00b6 ROCK is a tool that was created to solve a problem that was realized several years ago while engaged in real-world missions and training exercises. That problem was that the pervasive network sensor platform at the time, had many architectural and operational issues. It was built on an insecure platform, had performance problems, and did not follow the Unix Philosophy . The origin of RockNSM can be traced back to the Fall of 2014 when a couple of wide-eyed dreamers started working on their own solution while drinking whiskey in a hotel room. The project developed from an on-mission hasty replacement, to an all-in-one sensor solution, and now a more full featured analysis stack capable of multi-node deployments. Credit \u00b6 This project is made possible by the efforts of an ever-growing list of amazing people. Take a look around our project to see all our contributors.","title":"Backstory"},{"location":"about/backstory/#backstory","text":"ROCK is a tool that was created to solve a problem that was realized several years ago while engaged in real-world missions and training exercises. That problem was that the pervasive network sensor platform at the time, had many architectural and operational issues. It was built on an insecure platform, had performance problems, and did not follow the Unix Philosophy . The origin of RockNSM can be traced back to the Fall of 2014 when a couple of wide-eyed dreamers started working on their own solution while drinking whiskey in a hotel room. The project developed from an on-mission hasty replacement, to an all-in-one sensor solution, and now a more full featured analysis stack capable of multi-node deployments.","title":"Backstory"},{"location":"about/backstory/#credit","text":"This project is made possible by the efforts of an ever-growing list of amazing people. Take a look around our project to see all our contributors.","title":"Credit"},{"location":"about/dataflow/","text":"Data Flow \u00b6 This is a high level model of how packets flow through the sensor:","title":"Data Flow"},{"location":"about/dataflow/#data-flow","text":"This is a high level model of how packets flow through the sensor:","title":"Data Flow"},{"location":"about/what_is_it/","text":"What is ROCK \u00b6 The Mission \u00b6 Reliable - we believe the folks at Red Hat do Linux right. ROCK is built on Centos7 and provides an easy path to a supported enterprise OS ( RHEL ). Secure - with SELinux, ROCK is highly secure by default. SELinux uses context to define security controls to prevent, for instance, a text editor process from talking to the internet. #setenforce1 Scalable - Whether you're tapping a SoHo network or a large enterprise, ROCK is designed with scale in mind. Capability \u00b6 Passive and reliable high-speed data acquisition via AF_PACKET, feeding systems for metadata (Bro), signature detection (Suricata), extracted network file metadata (FSF), and full packet capture (Stenographer). A messaging layer (Kafka and Logstash) that provides flexibility in scaling the platform to meet operational needs, as well as providing some degree of data reliability in transit. Reliable data storage and indexing (Elasticsearch) to support rapid retrieval and analysis (Kibana and Docket) of the data. Pivoting off Kibana data rapidly into full packet capture (Docket and Stenographer). Components \u00b6 Full Packet Capture via Google Stenographer Protocol Analysis and Metadata via Bro Signature Based Alerting via Suricata Recursive File Scanning via FSF . Output from Suricata and FSF are moved to message queue via Filebeat Message Queuing and Distribution via Apache Kafka Message Transport via Logstash Data Storage, Indexing, and Search via Elasticsearch Analyst Toolkit \u00b6 Kibana provides data UI and visualization Docket allows for quick and targeted pivoting to PCAP","title":"What is ROCK"},{"location":"about/what_is_it/#what-is-rock","text":"","title":"What is ROCK"},{"location":"about/what_is_it/#the-mission","text":"Reliable - we believe the folks at Red Hat do Linux right. ROCK is built on Centos7 and provides an easy path to a supported enterprise OS ( RHEL ). Secure - with SELinux, ROCK is highly secure by default. SELinux uses context to define security controls to prevent, for instance, a text editor process from talking to the internet. #setenforce1 Scalable - Whether you're tapping a SoHo network or a large enterprise, ROCK is designed with scale in mind.","title":"The Mission"},{"location":"about/what_is_it/#capability","text":"Passive and reliable high-speed data acquisition via AF_PACKET, feeding systems for metadata (Bro), signature detection (Suricata), extracted network file metadata (FSF), and full packet capture (Stenographer). A messaging layer (Kafka and Logstash) that provides flexibility in scaling the platform to meet operational needs, as well as providing some degree of data reliability in transit. Reliable data storage and indexing (Elasticsearch) to support rapid retrieval and analysis (Kibana and Docket) of the data. Pivoting off Kibana data rapidly into full packet capture (Docket and Stenographer).","title":"Capability"},{"location":"about/what_is_it/#components","text":"Full Packet Capture via Google Stenographer Protocol Analysis and Metadata via Bro Signature Based Alerting via Suricata Recursive File Scanning via FSF . Output from Suricata and FSF are moved to message queue via Filebeat Message Queuing and Distribution via Apache Kafka Message Transport via Logstash Data Storage, Indexing, and Search via Elasticsearch","title":"Components"},{"location":"about/what_is_it/#analyst-toolkit","text":"Kibana provides data UI and visualization Docket allows for quick and targeted pivoting to PCAP","title":"Analyst Toolkit"},{"location":"configure/","text":"Configuration \u00b6 After installation your sensor configuration will need to be customized. This can be done by local login, or remotely via ssh. Config File \u00b6 The primary configuration file for ROCK is /etc/rocknsm/config.yml . This file contains key variables like network interface setup, cpu cores assignment, and much more. There are a lot of options to tune here, so take time to familiarize. Let's break down this file into it's major sections: Network Interface \u00b6 As mentioned previously, ROCK takes the interface with an ip address / gateway and will use that as the management NIC. Beginning at line 8, config.yml displays the remaining interfaces that will be used to MONITOR traffic. Let's run through a basic example: [admin@rock ~]$ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ... inet 192.168.1.207/24 brd 192.168.1.255 scope global noprefixroute dynamic enp0s3 ... 3: enp0s4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ... The demo box above has 2 NICs: 1. enp0s3 - is plugged in for install and deployment with an ip address from local dhcp. This will be used to manage the sensor 2. enp0s4 - will be unused (not connected) during install and deployment and be listed as a rock_monif in the config file Lines 7 - 9 of /etc/rocknsm/config.yml show that the other interface ( enp0s3 ) is listed as MONITOR interface. # interfaces that should be configured for sensor applications rock_monifs: - enp0s3 Sensor Resource \u00b6 # Set the hostname of the sensor: rock_hostname: # Set the Fully Qualified Domain Name: rock_fqdn: # Set the number of CPUs assigned to Bro: bro_cpu: # Set the Elasticsearch cluster name: es_cluster_name: # Set the Elasticsearch cluster node name: es_node_name: # Set the value of Elasticsearch memory: es_mem: Installation Source \u00b6 We've taken into consideration that your sensor won't always have internet access. The ISO's default value is set to offline: 53 # The primary installation variable defines the ROCK installation method: 54 # ONLINE: used if the system may reach out to the internet 55 # OFFLINE: used if the system may *NOT* reach out to the internet 56 # The default value \"False\" will deploy using OFFLINE (local) repos. 57 # A value of \"True\" will perform an install using ONLINE mirrors. 58 59 rock_online_install: True If your sensor has access to online repos just set rock_online_install: True , Ansible will configure your system for the yum repositories listed and pull packages and git repos directly from the URLs shown. You can easily point this to local mirrors if needed. If this value is set to False , Ansible will look at the cached files in /srv/rocknsm . Data Retention \u00b6 This section controls how long NSM data stay on the sensor: # Set the interval in which Elasticsearch indexes are closed: elastic_close_interval: # Set the interval in which Elasticsearch indexes are deleted: elastic_delete_interval: # Set value for Kafka retention (in hours): kafka_retention: # Set value for Bro log retention (in days): bro_log_retention: # Set value for Bro statistics log retention (in days): bro_stats_retention: # Set how often logrotate will roll Suricata log (in days): suricata_retention: # Set value for FSF log retention (in days): fsf_retention: ROCK Component Options \u00b6 This is a critical section that provides boolean options to choose what components of ROCK are installed and enabled during deployment. # The following \"with_\" statements define what components of RockNSM are # installed when running the deploy script: with_stenographer: True with_docket: True with_bro: True with_suricata: True with_snort: True with_suricata_update: True with_logstash: True with_elasticsearch: True with_kibana: True with_zookeeper: True with_kafka: True with_lighttpd: True with_fsf: True # The following \"enable_\" statements define what RockNSM component services # are enabled (start automatically on system boot): enable_stenographer: True enable_docket: True enable_bro: True enable_suricata: True enable_snort: True enable_suricata_update: True enable_logstash: True enable_elasticsearch: True enable_kibana: True enable_zookeeper: True enable_kafka: True enable_lighttpd: True enable_fsf: True A good example for changing this section would involve Stenographer . Collecting raw PCAP is resource and storage intensive . You're machine may not be able to handle that and if you just wanted to focus on network logs, then you would set both options in the config file to disable installing and enabling Stenographer: 67 with_stenographer: False ... ... ... 83 enable_stenographer: False","title":"Configure"},{"location":"configure/#configuration","text":"After installation your sensor configuration will need to be customized. This can be done by local login, or remotely via ssh.","title":"Configuration"},{"location":"configure/#config-file","text":"The primary configuration file for ROCK is /etc/rocknsm/config.yml . This file contains key variables like network interface setup, cpu cores assignment, and much more. There are a lot of options to tune here, so take time to familiarize. Let's break down this file into it's major sections:","title":"Config File"},{"location":"configure/#network-interface","text":"As mentioned previously, ROCK takes the interface with an ip address / gateway and will use that as the management NIC. Beginning at line 8, config.yml displays the remaining interfaces that will be used to MONITOR traffic. Let's run through a basic example: [admin@rock ~]$ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ... inet 192.168.1.207/24 brd 192.168.1.255 scope global noprefixroute dynamic enp0s3 ... 3: enp0s4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ... The demo box above has 2 NICs: 1. enp0s3 - is plugged in for install and deployment with an ip address from local dhcp. This will be used to manage the sensor 2. enp0s4 - will be unused (not connected) during install and deployment and be listed as a rock_monif in the config file Lines 7 - 9 of /etc/rocknsm/config.yml show that the other interface ( enp0s3 ) is listed as MONITOR interface. # interfaces that should be configured for sensor applications rock_monifs: - enp0s3","title":"Network Interface"},{"location":"configure/#sensor-resource","text":"# Set the hostname of the sensor: rock_hostname: # Set the Fully Qualified Domain Name: rock_fqdn: # Set the number of CPUs assigned to Bro: bro_cpu: # Set the Elasticsearch cluster name: es_cluster_name: # Set the Elasticsearch cluster node name: es_node_name: # Set the value of Elasticsearch memory: es_mem:","title":"Sensor Resource"},{"location":"configure/#installation-source","text":"We've taken into consideration that your sensor won't always have internet access. The ISO's default value is set to offline: 53 # The primary installation variable defines the ROCK installation method: 54 # ONLINE: used if the system may reach out to the internet 55 # OFFLINE: used if the system may *NOT* reach out to the internet 56 # The default value \"False\" will deploy using OFFLINE (local) repos. 57 # A value of \"True\" will perform an install using ONLINE mirrors. 58 59 rock_online_install: True If your sensor has access to online repos just set rock_online_install: True , Ansible will configure your system for the yum repositories listed and pull packages and git repos directly from the URLs shown. You can easily point this to local mirrors if needed. If this value is set to False , Ansible will look at the cached files in /srv/rocknsm .","title":"Installation Source"},{"location":"configure/#data-retention","text":"This section controls how long NSM data stay on the sensor: # Set the interval in which Elasticsearch indexes are closed: elastic_close_interval: # Set the interval in which Elasticsearch indexes are deleted: elastic_delete_interval: # Set value for Kafka retention (in hours): kafka_retention: # Set value for Bro log retention (in days): bro_log_retention: # Set value for Bro statistics log retention (in days): bro_stats_retention: # Set how often logrotate will roll Suricata log (in days): suricata_retention: # Set value for FSF log retention (in days): fsf_retention:","title":"Data Retention"},{"location":"configure/#rock-component-options","text":"This is a critical section that provides boolean options to choose what components of ROCK are installed and enabled during deployment. # The following \"with_\" statements define what components of RockNSM are # installed when running the deploy script: with_stenographer: True with_docket: True with_bro: True with_suricata: True with_snort: True with_suricata_update: True with_logstash: True with_elasticsearch: True with_kibana: True with_zookeeper: True with_kafka: True with_lighttpd: True with_fsf: True # The following \"enable_\" statements define what RockNSM component services # are enabled (start automatically on system boot): enable_stenographer: True enable_docket: True enable_bro: True enable_suricata: True enable_snort: True enable_suricata_update: True enable_logstash: True enable_elasticsearch: True enable_kibana: True enable_zookeeper: True enable_kafka: True enable_lighttpd: True enable_fsf: True A good example for changing this section would involve Stenographer . Collecting raw PCAP is resource and storage intensive . You're machine may not be able to handle that and if you just wanted to focus on network logs, then you would set both options in the config file to disable installing and enabling Stenographer: 67 with_stenographer: False ... ... ... 83 enable_stenographer: False","title":"ROCK Component Options"},{"location":"deployment/","text":"Deployment \u00b6 Deploy \u00b6 Once /etc/rocknsm/config.yml has been tuned to suit your environment, it's finally time to deploy this thing . This is done by running the deployment script, which is in the install user's path ( /usr/sbin/ ). To kick off the deployment script run: sudo deploy_rock.sh Once the deployment is completed with the components you chose, you'll be congratulated with a success banner. Generate Defaults \u00b6 What do I do when I've completely messed things up and need to start over? Great question. There's a simple solution for when the base config file needs to be reset back to default settings. There's a generate_defaults script also located in your $PATH . Simply execute this to regenerate a fresh default config file ( /etc/rocknsm/config.yml ) for you and get you out of jail: sudo generate_defaults.sh This effectively reverts things to factory defaults, and you can then revisit your config file. Initial Kibana Access \u00b6 We strive to do the little things right, so rather than having Kibana available to everyone in the free world, it's sitting behind a reverse proxy and secured by a passphrase . The credentials are generated and then stored in the home directory of the user you created during the initial installation e.g. /home/admin . To get into Kibana: cat and copy the contents of ~/KIBANA_CREDS.README browse to https:// enter this user / password combo profit!","title":"Deployment"},{"location":"deployment/#deployment","text":"","title":"Deployment"},{"location":"deployment/#deploy","text":"Once /etc/rocknsm/config.yml has been tuned to suit your environment, it's finally time to deploy this thing . This is done by running the deployment script, which is in the install user's path ( /usr/sbin/ ). To kick off the deployment script run: sudo deploy_rock.sh Once the deployment is completed with the components you chose, you'll be congratulated with a success banner.","title":"Deploy"},{"location":"deployment/#generate-defaults","text":"What do I do when I've completely messed things up and need to start over? Great question. There's a simple solution for when the base config file needs to be reset back to default settings. There's a generate_defaults script also located in your $PATH . Simply execute this to regenerate a fresh default config file ( /etc/rocknsm/config.yml ) for you and get you out of jail: sudo generate_defaults.sh This effectively reverts things to factory defaults, and you can then revisit your config file.","title":"Generate Defaults"},{"location":"deployment/#initial-kibana-access","text":"We strive to do the little things right, so rather than having Kibana available to everyone in the free world, it's sitting behind a reverse proxy and secured by a passphrase . The credentials are generated and then stored in the home directory of the user you created during the initial installation e.g. /home/admin . To get into Kibana: cat and copy the contents of ~/KIBANA_CREDS.README browse to https:// enter this user / password combo profit!","title":"Initial Kibana Access"},{"location":"install/install/","text":"Installation Guide \u00b6 Network Connection \u00b6 Before starting, let's cover a critical setup point: Before initial boot, connect the network interface that you intend to use to remotely manage ROCK. During install, ROCK will see the network interface with an ip address and default gateway and designate it as the management port. So with that info, plug into that interface and boot to your USB. Install Types \u00b6 ROCK works with both legacy BIOS and UEFI booting. Once booted from the USB, you are presented with 2 primary installation paths: Automated Custom Automated \u00b6 The \"Automated\" option is intended to serve as a starting point that allows you to get into things. It uses the Centos Anaconda installer to make some of the harder decisions for users by skipping over many options to get you up and running. It makes a best guess at how to use resources -- most notably how to manage available disks. Bottom line: think of this as a product quickstart mode, perfect for installing on a VM or other temporary hardware. It is not for production sensor deployment. For the rest of this install guide we'll work through the more detail oriented \"Custom Install of ROCK\" option. Custom \u00b6 The \"Custom\" allows for more customization of a ROCK installation. This is especially helpful when you're working with multiple disks or even a large amount of storage on a single disk. The Custom option is recommended for production environments in order to get more granular in choosing how disk space is allocated. Disk Allocation \u00b6 Configuring disk and storage is a deep topic on it's own, but let's talke about a few examples to get started: Stenographer \u00b6 A common gotcha occurs when you want full packet capture (via Stenographer ), but it isn't given a separate partition. Stenographer is great at managing it's own disk space (starts to overwrite oldest data at 90% capacity), but that doesn't cut it when it's sharing the same mount point as Bro, Suricata , and other tools that generate data in ROCK. Best practice would be to create a /data/stenographer partition in order to prevent limited operations. For example, Elasticsearch will (rightfully) lock indexes up to a read-only state in order to keep things from crashing hard. System Logs \u00b6 Another useful partition to create is /var/log to separate system log files from the rest of the system. Example Table \u00b6 Below is a good starting point when partitioning MOUNT POINT USAGE SIZE SYSTEM SYSTEM SYSTEM / root filesystem 15 GiB /boot legacy boot files 512 MiB /boot/efi uefi boot files 512 MiB swap memory shortage ~8 GiB+ DATA DATA DATA /var/log system log files ~15 GiB /home user home dirs ~20 GiB /data data partition ~ GiB /data/stenographer steno partition ~ GiB For more information to assist with the partitioning process, you can see the RHEL guide . Also, it may be a bit more self explanatory for you if you click \u201cautomatic partitions\u201d then modify accordingly. Date & Time \u00b6 UTC is generally preferred for logging data as the timestamps from anywhere in the world will have a proper order without calculating offsets and daylight savings. That said, Kibana will present the Bro logs according to your timezone (as set in the browser). The bro logs themselves (i.e. in /data/bro/logs/) log in epoch time and will be written in UTC regardless of the system timezone. Bro includes a utility for parsing these on the command line called bro-cut . It can be used to print human-readable timestamps in either the local sensor timezone or UTC. You can also give it a custom format string to specify what you'd like displayed. Network & Hostname \u00b6 Before beginning the install process it's best to connect the interface you've selected to be the management interface . Here's the order of events: ROCK will initially look for an interface with a default gateway and treat that interface as the MGMT INTERFACE All remaining interfaces will be treated as MONITOR INTERFACES Ensure that the interface you intend to use for MGMT has been turned on and has an ip address Set the hostname of the sensor in the bottom left corner this hostname will populate the Ansible inventory file in /etc/rocknsm/hosts.ini User Creation \u00b6 ROCK is configured with the root user disabled. We recommend that you leave it that way. Once you've kicked off the install, click User Creation at the next screen (shown above) and complete the required fields to set up a non-root admin user. If this step is not completed during install, do not fear. you will be prompted to create this account after first login. Wrapping Up \u00b6 Once the install is complete you will be able to click Finish Installation and then reboot. You can then accept the license agreement: c (ontinue) + ENTER The sshd services is enabled at startup, so if you intend to complete the next steps remotely, note the management ip address now by running ip a .","title":"Installation"},{"location":"install/install/#installation-guide","text":"","title":"Installation Guide"},{"location":"install/install/#network-connection","text":"Before starting, let's cover a critical setup point: Before initial boot, connect the network interface that you intend to use to remotely manage ROCK. During install, ROCK will see the network interface with an ip address and default gateway and designate it as the management port. So with that info, plug into that interface and boot to your USB.","title":"Network Connection"},{"location":"install/install/#install-types","text":"ROCK works with both legacy BIOS and UEFI booting. Once booted from the USB, you are presented with 2 primary installation paths: Automated Custom","title":"Install Types"},{"location":"install/install/#automated","text":"The \"Automated\" option is intended to serve as a starting point that allows you to get into things. It uses the Centos Anaconda installer to make some of the harder decisions for users by skipping over many options to get you up and running. It makes a best guess at how to use resources -- most notably how to manage available disks. Bottom line: think of this as a product quickstart mode, perfect for installing on a VM or other temporary hardware. It is not for production sensor deployment. For the rest of this install guide we'll work through the more detail oriented \"Custom Install of ROCK\" option.","title":"Automated"},{"location":"install/install/#custom","text":"The \"Custom\" allows for more customization of a ROCK installation. This is especially helpful when you're working with multiple disks or even a large amount of storage on a single disk. The Custom option is recommended for production environments in order to get more granular in choosing how disk space is allocated.","title":"Custom"},{"location":"install/install/#disk-allocation","text":"Configuring disk and storage is a deep topic on it's own, but let's talke about a few examples to get started:","title":"Disk Allocation"},{"location":"install/install/#stenographer","text":"A common gotcha occurs when you want full packet capture (via Stenographer ), but it isn't given a separate partition. Stenographer is great at managing it's own disk space (starts to overwrite oldest data at 90% capacity), but that doesn't cut it when it's sharing the same mount point as Bro, Suricata , and other tools that generate data in ROCK. Best practice would be to create a /data/stenographer partition in order to prevent limited operations. For example, Elasticsearch will (rightfully) lock indexes up to a read-only state in order to keep things from crashing hard.","title":"Stenographer"},{"location":"install/install/#system-logs","text":"Another useful partition to create is /var/log to separate system log files from the rest of the system.","title":"System Logs"},{"location":"install/install/#example-table","text":"Below is a good starting point when partitioning MOUNT POINT USAGE SIZE SYSTEM SYSTEM SYSTEM / root filesystem 15 GiB /boot legacy boot files 512 MiB /boot/efi uefi boot files 512 MiB swap memory shortage ~8 GiB+ DATA DATA DATA /var/log system log files ~15 GiB /home user home dirs ~20 GiB /data data partition ~ GiB /data/stenographer steno partition ~ GiB For more information to assist with the partitioning process, you can see the RHEL guide . Also, it may be a bit more self explanatory for you if you click \u201cautomatic partitions\u201d then modify accordingly.","title":"Example Table"},{"location":"install/install/#date-time","text":"UTC is generally preferred for logging data as the timestamps from anywhere in the world will have a proper order without calculating offsets and daylight savings. That said, Kibana will present the Bro logs according to your timezone (as set in the browser). The bro logs themselves (i.e. in /data/bro/logs/) log in epoch time and will be written in UTC regardless of the system timezone. Bro includes a utility for parsing these on the command line called bro-cut . It can be used to print human-readable timestamps in either the local sensor timezone or UTC. You can also give it a custom format string to specify what you'd like displayed.","title":"Date &amp; Time"},{"location":"install/install/#network-hostname","text":"Before beginning the install process it's best to connect the interface you've selected to be the management interface . Here's the order of events: ROCK will initially look for an interface with a default gateway and treat that interface as the MGMT INTERFACE All remaining interfaces will be treated as MONITOR INTERFACES Ensure that the interface you intend to use for MGMT has been turned on and has an ip address Set the hostname of the sensor in the bottom left corner this hostname will populate the Ansible inventory file in /etc/rocknsm/hosts.ini","title":"Network &amp; Hostname"},{"location":"install/install/#user-creation","text":"ROCK is configured with the root user disabled. We recommend that you leave it that way. Once you've kicked off the install, click User Creation at the next screen (shown above) and complete the required fields to set up a non-root admin user. If this step is not completed during install, do not fear. you will be prompted to create this account after first login.","title":"User Creation"},{"location":"install/install/#wrapping-up","text":"Once the install is complete you will be able to click Finish Installation and then reboot. You can then accept the license agreement: c (ontinue) + ENTER The sshd services is enabled at startup, so if you intend to complete the next steps remotely, note the management ip address now by running ip a .","title":"Wrapping Up"},{"location":"install/media/","text":"Install Media \u00b6 If there\u2019s one thing that should be carried away from the installation section, it's this: RockNSM has been designed to be used as a security distribution, not a package or a suite of tools. It\u2019s built from the ground up and the ONLY SUPPORTED INSTALL IS THE OFFICIAL ISO. Yes, one can clone the project and run the Ansible on some bespoke CentOS build, and you may have great success... but you've voided the warranty . Providing a clean product that makes supporting submitted issues is important to us. The ISO addresses most use cases. Download \u00b6 The lastest ROCK build is available at download.rocknsm.io . Applying the ISO \u00b6 Now it's time to create a bootable USB drive with the fresh ROCK build. Let's look at few options. Linux \u00b6 CLI \u00b6 If you live in the terminal, use dd to apply the image. These instructions are for using a RHEL based system. If you're in a different environment, google is your friend. CAUTION when using these commands by ENSURING you're writing to the correct disk / partition! once you've inserted a USB get the drive ID: lsblk unmount the target drive so you can write to it: umount /dev/disk# write the image to drive: sudo dd bs=8M if=path/to/rockiso of=/dev/disk# GUI \u00b6 If you don't want to apply the image in the terminal, there are plenty of great tools to do this with a graphical interface: Etcher - our go-to choice (cross-platform) SD Card Formatter - works well YUMI - create multibooting disk macOS \u00b6 CLI \u00b6 For the terminal, we'll once again use dd , but with a few differences from the linux instructions above. CAUTION when using these commands by ENSURING you're writing to the correct disk / partition! once you've inserted a USB get the drive ID: diskutil list unmount the target drive so you can write to it: diskutil unmount /dev/disk# write the image to drive: sudo dd bs=8m if=path/to/rockiso of=/dev/disk# GUI \u00b6 Etcher - our go-to choice (cross-platform) Windows \u00b6 When applying ISO on a Windows box our experience is entirely in graphical applications. Here's a list of what works well: Etcher - our go-to standard Win32 Disk Imager SD Card Formatter","title":"Media"},{"location":"install/media/#install-media","text":"If there\u2019s one thing that should be carried away from the installation section, it's this: RockNSM has been designed to be used as a security distribution, not a package or a suite of tools. It\u2019s built from the ground up and the ONLY SUPPORTED INSTALL IS THE OFFICIAL ISO. Yes, one can clone the project and run the Ansible on some bespoke CentOS build, and you may have great success... but you've voided the warranty . Providing a clean product that makes supporting submitted issues is important to us. The ISO addresses most use cases.","title":"Install Media"},{"location":"install/media/#download","text":"The lastest ROCK build is available at download.rocknsm.io .","title":"Download"},{"location":"install/media/#applying-the-iso","text":"Now it's time to create a bootable USB drive with the fresh ROCK build. Let's look at few options.","title":"Applying the ISO"},{"location":"install/media/#linux","text":"","title":"Linux"},{"location":"install/media/#cli","text":"If you live in the terminal, use dd to apply the image. These instructions are for using a RHEL based system. If you're in a different environment, google is your friend. CAUTION when using these commands by ENSURING you're writing to the correct disk / partition! once you've inserted a USB get the drive ID: lsblk unmount the target drive so you can write to it: umount /dev/disk# write the image to drive: sudo dd bs=8M if=path/to/rockiso of=/dev/disk#","title":"CLI"},{"location":"install/media/#gui","text":"If you don't want to apply the image in the terminal, there are plenty of great tools to do this with a graphical interface: Etcher - our go-to choice (cross-platform) SD Card Formatter - works well YUMI - create multibooting disk","title":"GUI"},{"location":"install/media/#macos","text":"","title":"macOS"},{"location":"install/media/#cli_1","text":"For the terminal, we'll once again use dd , but with a few differences from the linux instructions above. CAUTION when using these commands by ENSURING you're writing to the correct disk / partition! once you've inserted a USB get the drive ID: diskutil list unmount the target drive so you can write to it: diskutil unmount /dev/disk# write the image to drive: sudo dd bs=8m if=path/to/rockiso of=/dev/disk#","title":"CLI"},{"location":"install/media/#gui_1","text":"Etcher - our go-to choice (cross-platform)","title":"GUI"},{"location":"install/media/#windows","text":"When applying ISO on a Windows box our experience is entirely in graphical applications. Here's a list of what works well: Etcher - our go-to standard Win32 Disk Imager SD Card Formatter","title":"Windows"},{"location":"install/requirements/","text":"Requirements \u00b6 Installation of ROCK can be broken down into three main steps: install configure deploy Before that, let's cover what you're going to need before starting. Sensor Hardware \u00b6 The analysis of live network data is a resource intensive task. The bottom line is this: if you throw hardware at ROCK it will use it, and use it well . The higher the IOPS the better. Here's a starting point to get you moving: RESOURCE RECOMMENDATION CPU 4 or more physical cores Memory 16GB ( 8GB to start, more the better ) Storage 256GB, with 200+ of that dedicated to /data , SSD preferred Network 2 gigabit interfaces, one for management and one for collection Install Media \u00b6 ROCK install image - download .iso here 8GB+ capacity USB drive - to apply install image BIOS settings to allow booting from mounted USB drive Network Connection \u00b6 ROCK is first and foremost a passive network sensor and is designed with the assumption that there may not be a network connection available during install. There's some built in flexibility on how This will be clarified more in then next sections. NOTE: Check out the ROCK@home Video Series that goes into detail on many things about deploying ROCK to include hardware choices for both sensor and network equipment.","title":"Requirements"},{"location":"install/requirements/#requirements","text":"Installation of ROCK can be broken down into three main steps: install configure deploy Before that, let's cover what you're going to need before starting.","title":"Requirements"},{"location":"install/requirements/#sensor-hardware","text":"The analysis of live network data is a resource intensive task. The bottom line is this: if you throw hardware at ROCK it will use it, and use it well . The higher the IOPS the better. Here's a starting point to get you moving: RESOURCE RECOMMENDATION CPU 4 or more physical cores Memory 16GB ( 8GB to start, more the better ) Storage 256GB, with 200+ of that dedicated to /data , SSD preferred Network 2 gigabit interfaces, one for management and one for collection","title":"Sensor Hardware"},{"location":"install/requirements/#install-media","text":"ROCK install image - download .iso here 8GB+ capacity USB drive - to apply install image BIOS settings to allow booting from mounted USB drive","title":"Install Media"},{"location":"install/requirements/#network-connection","text":"ROCK is first and foremost a passive network sensor and is designed with the assumption that there may not be a network connection available during install. There's some built in flexibility on how This will be clarified more in then next sections. NOTE: Check out the ROCK@home Video Series that goes into detail on many things about deploying ROCK to include hardware choices for both sensor and network equipment.","title":"Network Connection"},{"location":"install/vm_installation/","text":"VM Build Guide \u00b6 The following walkthrough is based on VMware Fusion, but serves well as a general template to follow. The more resources you give ROCK, the happier it'll be. New VM \u00b6 in the top left corner click \" Add > New... then Custom Machine \" select the \" Linux > RedHat Enterprise 64 template \" create new virtual disk name your VM, save Lets customize some settings, change based on your hardware. Processors & Memory Processors - 4 cores Memory - 8192MB (8GB) Hard Disk increase the disk to 20GB customize settings save as name Network Adapter By default the vm is created with one interface - this will be for management. lets add a second (listening) interface: add device (top right), net adapter, add, \u201cprivate to my mac\u201d Boot Device click CD/DVD (IDE) check the \"Connect CD/DVD Drive\" box expand advanced options and browse to the latest ROCK iso Run Auto Installer Once the above changes are made, it's time to install the OS. Lets run the \"Auto Install\", but before we do, there are some customization that can be done for VMs: click the \" Start Up \" button while holding the esc key hit tab for full config options add the following values, speparated by spaces: biosdevname=0 net.ifnames=0 - this will ensure you get interface names like eth0 . If you have physical hardware, I highly recommend that you do not use this function vga=773 - improves video resolution issues ENTER , and ROCK install script will install create admin user acct REBOOT when install process is complete TIP: The root account is locked by default and the user account you created has sudo access. Updating \u00b6 NOTE: VMware Fusion will allow local ssh out of the box. If you're using Virtualbox you'll need to set up local ssh port forwarding . Log in with the admin credentials used during the install process, and lets get this box current: sudo yum update -y && reboot Now you're ready to move on to Usage Guide .","title":"VM Build Guide"},{"location":"install/vm_installation/#vm-build-guide","text":"The following walkthrough is based on VMware Fusion, but serves well as a general template to follow. The more resources you give ROCK, the happier it'll be.","title":"VM Build Guide"},{"location":"install/vm_installation/#new-vm","text":"in the top left corner click \" Add > New... then Custom Machine \" select the \" Linux > RedHat Enterprise 64 template \" create new virtual disk name your VM, save Lets customize some settings, change based on your hardware. Processors & Memory Processors - 4 cores Memory - 8192MB (8GB) Hard Disk increase the disk to 20GB customize settings save as name Network Adapter By default the vm is created with one interface - this will be for management. lets add a second (listening) interface: add device (top right), net adapter, add, \u201cprivate to my mac\u201d Boot Device click CD/DVD (IDE) check the \"Connect CD/DVD Drive\" box expand advanced options and browse to the latest ROCK iso Run Auto Installer Once the above changes are made, it's time to install the OS. Lets run the \"Auto Install\", but before we do, there are some customization that can be done for VMs: click the \" Start Up \" button while holding the esc key hit tab for full config options add the following values, speparated by spaces: biosdevname=0 net.ifnames=0 - this will ensure you get interface names like eth0 . If you have physical hardware, I highly recommend that you do not use this function vga=773 - improves video resolution issues ENTER , and ROCK install script will install create admin user acct REBOOT when install process is complete TIP: The root account is locked by default and the user account you created has sudo access.","title":"New VM"},{"location":"install/vm_installation/#updating","text":"NOTE: VMware Fusion will allow local ssh out of the box. If you're using Virtualbox you'll need to set up local ssh port forwarding . Log in with the admin credentials used during the install process, and lets get this box current: sudo yum update -y && reboot Now you're ready to move on to Usage Guide .","title":"Updating"},{"location":"reference/changelog/","text":"Changelog \u00b6 2.3 -- 2019-02-25 \u00b6 New: Add ability to do multi-host deployment of sensor + data tiers (#339) New: Integrate Docket into Kibana by default New: Improvements and additional Kibana dashboards Fixes: issue with Bro failing when monitor interface is down (#343) Fixes: issue with services starting that shouldn\u2019t (#346) Fixes: race condition on loading dashboards into Kibana (#356) Fixes: configuration for Docket allowing serving from non-root URI (#361) Change: bro log retention value to one week rather than forever (#345) Change: Greatly improve documentation (#338) Change: Reorganize README (#308) Change: Move ECS to rock-dashboards repo (#305) Change: Move RockNSM install paths to filesystem hierarchy standard locations (#344) 2.2 -- 2018-10-26 \u00b6 Feature: rockctl command to quickly check or change services Feature: Docket, a REST API and web UI to query multiple stenographer instances, now using TCP port 443 Optimization: Kibana is now running on TCP port 443 Feature: Added Suricata-Update to manage Suricata signatures Feature: GPG signing of packages and repo metadata Feature: Added functional tests using testinfra Feature: Initial support of Elastic Common Schema Feature: Elastic new Features Canvas Elastic Maps Service Feature: Include full Elasticstack (with permission) including features formerly known as X-Pack: Graph Machine Learning Reporting Security Monitoring Alerting Elasticsearch SQL Optimization: Elastic dashboards, mappings, and Logstash config moved to module-like construct Upgrade: CentOS is updated to 7.5 (1804) Upgrade: Elastic Stack is updated to 6.4.2 Upgrade: Suricata is updated to 4.0.5 Upgrade: Bro is updated to 2.5.4 2.1 -- 2018-08-23 \u00b6","title":"Changelog"},{"location":"reference/changelog/#changelog","text":"","title":"Changelog"},{"location":"reference/changelog/#23-2019-02-25","text":"New: Add ability to do multi-host deployment of sensor + data tiers (#339) New: Integrate Docket into Kibana by default New: Improvements and additional Kibana dashboards Fixes: issue with Bro failing when monitor interface is down (#343) Fixes: issue with services starting that shouldn\u2019t (#346) Fixes: race condition on loading dashboards into Kibana (#356) Fixes: configuration for Docket allowing serving from non-root URI (#361) Change: bro log retention value to one week rather than forever (#345) Change: Greatly improve documentation (#338) Change: Reorganize README (#308) Change: Move ECS to rock-dashboards repo (#305) Change: Move RockNSM install paths to filesystem hierarchy standard locations (#344)","title":"2.3 -- 2019-02-25"},{"location":"reference/changelog/#22-2018-10-26","text":"Feature: rockctl command to quickly check or change services Feature: Docket, a REST API and web UI to query multiple stenographer instances, now using TCP port 443 Optimization: Kibana is now running on TCP port 443 Feature: Added Suricata-Update to manage Suricata signatures Feature: GPG signing of packages and repo metadata Feature: Added functional tests using testinfra Feature: Initial support of Elastic Common Schema Feature: Elastic new Features Canvas Elastic Maps Service Feature: Include full Elasticstack (with permission) including features formerly known as X-Pack: Graph Machine Learning Reporting Security Monitoring Alerting Elasticsearch SQL Optimization: Elastic dashboards, mappings, and Logstash config moved to module-like construct Upgrade: CentOS is updated to 7.5 (1804) Upgrade: Elastic Stack is updated to 6.4.2 Upgrade: Suricata is updated to 4.0.5 Upgrade: Bro is updated to 2.5.4","title":"2.2 -- 2018-10-26"},{"location":"reference/changelog/#21-2018-08-23","text":"","title":"2.1 -- 2018-08-23"},{"location":"reference/contribution/","text":"How to Pitch In \u00b6 ROCK is an open source project and would not be what it is without a community of users and contributors. There are many ways to contribute, so take a look at how: General Support \u00b6 For quick questions and deployment support, please join the RockNSM Community . Github Contribution \u00b6 Issues \u00b6 Before you submit an issue please search the issue tracker, as there may already be an issue for your problem and it's discussion might just solve things. We want to resolve issues as soon as possible, but in order to do so please provide as much detail about the environment and situation as possible. Pull Requests \u00b6 In order to issue a PR, fork the project, make your changes, and add descriptive messages to your commits (this is a mandatory requirement for your PR to be considered). Submit a PR to the main rock repo . If changes or additions are suggested, edit your fork and this will automatically update your PR.","title":"Contribution"},{"location":"reference/contribution/#how-to-pitch-in","text":"ROCK is an open source project and would not be what it is without a community of users and contributors. There are many ways to contribute, so take a look at how:","title":"How to Pitch In"},{"location":"reference/contribution/#general-support","text":"For quick questions and deployment support, please join the RockNSM Community .","title":"General Support"},{"location":"reference/contribution/#github-contribution","text":"","title":"Github Contribution"},{"location":"reference/contribution/#issues","text":"Before you submit an issue please search the issue tracker, as there may already be an issue for your problem and it's discussion might just solve things. We want to resolve issues as soon as possible, but in order to do so please provide as much detail about the environment and situation as possible.","title":"Issues"},{"location":"reference/contribution/#pull-requests","text":"In order to issue a PR, fork the project, make your changes, and add descriptive messages to your commits (this is a mandatory requirement for your PR to be considered). Submit a PR to the main rock repo . If changes or additions are suggested, edit your fork and this will automatically update your PR.","title":"Pull Requests"},{"location":"reference/latest/","text":"Latest \u00b6 Release 2.3 \u00b6 We are pleased to announce that ROCK 2.3 is here! The RockNSM team has been hard at work lately trying to get into a more regular cadence for releases. While RockNSM 2.2 was a relatively small release, 2.3 comes with a lot of changes. You can read the full details in the changelog , but here's a quick overview of some of the latest additions: Support for Elastic 7 pre-release Bro 2.6, Suricata 4.2, Elastic 6.6, plus the latest JA3 and ET rules Query PCAP directly from Kibana via Docket Multi-node support \ud83d\ude4c Artifact restructuring 61 closed issues (including a lot of outdated items) ~ Elastic 7 Support \u00b6 We have been performing some early testing with the pre-release versions of Elastic version 7 , and are excited to share that ROCK 2.3 includes support for running on the pre-release version if you would like to do some of your own testing. Package Updates \u00b6 This update brings Bro to version 2.6.1, which enables additional protocol analyzers and a lot more event hooks. You can see the full list of changes here . Suricata has been updated to 4.2 and includes the full Suricata protocol analyzer suite, which has some additional coverage for ICS/SCADA stuff beyond what Bro provides. Multi-node Support \u00b6 This is a big change that we are very excited to release! When ROCK was originally conceived, its primary purpose was for Network Security Monitoring (NSM) practitioners to be able to hone their craft on a commodity home lab. While that is still a very relevant use case, we have seen a lot of demand for the ability to easily deploy ROCK into an enterprise environment in a scalable manner. Look for another blog in the near future to walk through the multi-node changes in more detail. Artifact Restructuring \u00b6 In recent versions of ROCK we stored most installation items under /opt/rocknsm/rock . With 2.3 this changes to the more appropriate POSIX compliant file paths. See here for more details. Closed Issues \u00b6 This comes back to having full-time staff that are able to dedicate time to the project. We are working hard to close out legacy issues, which will make new issues much easier to triage.","title":"Latest Release"},{"location":"reference/latest/#latest","text":"","title":"Latest"},{"location":"reference/latest/#release-23","text":"We are pleased to announce that ROCK 2.3 is here! The RockNSM team has been hard at work lately trying to get into a more regular cadence for releases. While RockNSM 2.2 was a relatively small release, 2.3 comes with a lot of changes. You can read the full details in the changelog , but here's a quick overview of some of the latest additions: Support for Elastic 7 pre-release Bro 2.6, Suricata 4.2, Elastic 6.6, plus the latest JA3 and ET rules Query PCAP directly from Kibana via Docket Multi-node support \ud83d\ude4c Artifact restructuring 61 closed issues (including a lot of outdated items) ~","title":"Release 2.3"},{"location":"reference/latest/#elastic-7-support","text":"We have been performing some early testing with the pre-release versions of Elastic version 7 , and are excited to share that ROCK 2.3 includes support for running on the pre-release version if you would like to do some of your own testing.","title":"Elastic 7 Support"},{"location":"reference/latest/#package-updates","text":"This update brings Bro to version 2.6.1, which enables additional protocol analyzers and a lot more event hooks. You can see the full list of changes here . Suricata has been updated to 4.2 and includes the full Suricata protocol analyzer suite, which has some additional coverage for ICS/SCADA stuff beyond what Bro provides.","title":"Package Updates"},{"location":"reference/latest/#multi-node-support","text":"This is a big change that we are very excited to release! When ROCK was originally conceived, its primary purpose was for Network Security Monitoring (NSM) practitioners to be able to hone their craft on a commodity home lab. While that is still a very relevant use case, we have seen a lot of demand for the ability to easily deploy ROCK into an enterprise environment in a scalable manner. Look for another blog in the near future to walk through the multi-node changes in more detail.","title":"Multi-node Support"},{"location":"reference/latest/#artifact-restructuring","text":"In recent versions of ROCK we stored most installation items under /opt/rocknsm/rock . With 2.3 this changes to the more appropriate POSIX compliant file paths. See here for more details.","title":"Artifact Restructuring"},{"location":"reference/latest/#closed-issues","text":"This comes back to having full-time staff that are able to dedicate time to the project. We are working hard to close out legacy issues, which will make new issues much easier to triage.","title":"Closed Issues"},{"location":"reference/liscense/","text":"Liscense \u00b6 Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Liscense"},{"location":"reference/liscense/#liscense","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Liscense"},{"location":"reference/tutorials/","text":"Tutorials \u00b6 We've been working hard to create clear and relevant training content: Video Guides \u00b6 ROCK Introduction \u00b6 ROCK Introduction - what ROCK is and how everything works together ROCK@home Series \u00b6 ROCK@home - 3 part series on the lowest barrier to entry: tapping your home network BSidesKC 2018 \u00b6 Threat Hunting with RockNSM - this talk by Bradford Dabbs discusses the benefits of a passive first approach and how RockNSM can be used to facilitate it.","title":"Tutorials and Videos"},{"location":"reference/tutorials/#tutorials","text":"We've been working hard to create clear and relevant training content:","title":"Tutorials"},{"location":"reference/tutorials/#video-guides","text":"","title":"Video Guides"},{"location":"reference/tutorials/#rock-introduction","text":"ROCK Introduction - what ROCK is and how everything works together","title":"ROCK Introduction"},{"location":"reference/tutorials/#rockhome-series","text":"ROCK@home - 3 part series on the lowest barrier to entry: tapping your home network","title":"ROCK@home Series"},{"location":"reference/tutorials/#bsideskc-2018","text":"Threat Hunting with RockNSM - this talk by Bradford Dabbs discusses the benefits of a passive first approach and how RockNSM can be used to facilitate it.","title":"BSidesKC 2018"},{"location":"services/","text":"Summary \u00b6 ROCK uses a collection of open-source applications as described in this Services section. This portion of the guide covers basic administration of each part of RockNSM. Published URLs and Ports \u00b6 ROCK uses the lighttpd webserver to perform vhost redirects to it's web interfaces. It's configured to listen for (IPV4 only) connections over port 443 for the following: URLs \u00b6 Kibana is accessible at: https://<sensorip>/app/kibana Docket is accessible at: https://<sensorip>/app/docket/ Ports \u00b6 Elasticsearch: :9200 Kibana: :5601 Rockctl \u00b6 If you're generally familiar with systemd you'll know that the command used to manage services on modern systems is systemctl . With ROCK we've provided a wrapper to control services called rockctl . Rockctl can be used to perform the following operations: $ sudo rockctl status # display status of ROCK services start # start all ROCK services stop # stop all ROCK services reset-failed # clear the failed states of services Here is an example output of rockctl status : [ admin@rock ~ ] $ sudo rockctl status ZOOKEEPER: Active: active ( running ) since Mon 2019 -01-28 21 :35:55 UTC ; 1 weeks 0 days ago KAFKA: Active: active ( running ) since Mon 2019 -01-28 21 :35:55 UTC ; 1 weeks 0 days ago BRO: Active: active ( running ) since Mon 2019 -01-28 21 :44:36 UTC ; 1 weeks 0 days ago SURICATA: Active: active ( running ) since Mon 2019 -01-28 21 :40:21 UTC ; 1 weeks 0 days ago FILEBEAT: Active: active ( running ) since Mon 2019 -01-28 21 :36:26 UTC ; 1 weeks 0 days ago ELASTICSEARCH: Active: active ( running ) since Mon 2019 -01-28 21 :35:57 UTC ; 1 weeks 0 days ago LOGSTASH: Active: active ( running ) since Mon 2019 -01-28 21 :36:26 UTC ; 1 weeks 0 days ago KIBANA: Active: active ( running ) since Mon 2019 -01-28 21 :36:10 UTC ; 1 weeks 0 days ago STENOGRAPHER: Active: active ( exited ) since Mon 2019 -01-28 22 :51:47 UTC ; 1 weeks 0 days ago STENOGRAPHER@EM1: Active: active ( running ) since Mon 2019 -01-28 22 :51:47 UTC ; 1 weeks 0 days ago","title":"Overview"},{"location":"services/#summary","text":"ROCK uses a collection of open-source applications as described in this Services section. This portion of the guide covers basic administration of each part of RockNSM.","title":"Summary"},{"location":"services/#published-urls-and-ports","text":"ROCK uses the lighttpd webserver to perform vhost redirects to it's web interfaces. It's configured to listen for (IPV4 only) connections over port 443 for the following:","title":"Published URLs and Ports"},{"location":"services/#urls","text":"Kibana is accessible at: https://<sensorip>/app/kibana Docket is accessible at: https://<sensorip>/app/docket/","title":"URLs"},{"location":"services/#ports","text":"Elasticsearch: :9200 Kibana: :5601","title":"Ports"},{"location":"services/#rockctl","text":"If you're generally familiar with systemd you'll know that the command used to manage services on modern systems is systemctl . With ROCK we've provided a wrapper to control services called rockctl . Rockctl can be used to perform the following operations: $ sudo rockctl status # display status of ROCK services start # start all ROCK services stop # stop all ROCK services reset-failed # clear the failed states of services Here is an example output of rockctl status : [ admin@rock ~ ] $ sudo rockctl status ZOOKEEPER: Active: active ( running ) since Mon 2019 -01-28 21 :35:55 UTC ; 1 weeks 0 days ago KAFKA: Active: active ( running ) since Mon 2019 -01-28 21 :35:55 UTC ; 1 weeks 0 days ago BRO: Active: active ( running ) since Mon 2019 -01-28 21 :44:36 UTC ; 1 weeks 0 days ago SURICATA: Active: active ( running ) since Mon 2019 -01-28 21 :40:21 UTC ; 1 weeks 0 days ago FILEBEAT: Active: active ( running ) since Mon 2019 -01-28 21 :36:26 UTC ; 1 weeks 0 days ago ELASTICSEARCH: Active: active ( running ) since Mon 2019 -01-28 21 :35:57 UTC ; 1 weeks 0 days ago LOGSTASH: Active: active ( running ) since Mon 2019 -01-28 21 :36:26 UTC ; 1 weeks 0 days ago KIBANA: Active: active ( running ) since Mon 2019 -01-28 21 :36:10 UTC ; 1 weeks 0 days ago STENOGRAPHER: Active: active ( exited ) since Mon 2019 -01-28 22 :51:47 UTC ; 1 weeks 0 days ago STENOGRAPHER@EM1: Active: active ( running ) since Mon 2019 -01-28 22 :51:47 UTC ; 1 weeks 0 days ago","title":"Rockctl"},{"location":"services/bro/","text":"Bro \u00b6 Overview \u00b6 Bro is used to provide network protocol analysis within ROCK. It is extremely customizable, and it is encouraged that you take advantage of this. When deploying custom Bro scripts, please be sure to store them under a subdirectory of /usr/share/bro/site/scripts/ . We can't guarantee that your customizations won't be overwritten by Ansible if you don't follow this pattern. Management \u00b6 Service \u00b6 Bro is deployed as a systemd unit, called bro . Normal systemd procedures apply here: sudo systemctl start bro sudo systemctl status bro sudo systemctl stop bro sudo systemctl restart bro The broctl command is now an alias. Using this alias prevents dangerous permission changes caused by running the real broctl binary with sudo. The only safe way otherwise to run broctl is to execute it as the bro user and bro group as such: `sudo -u bro -g bro /usr/bin/broctl` Directories \u00b6 Home /usr/share/bro/ Data /data/bro/logs/current/{stream_name.log} Application Logs /data/bro/logs/current/{stdout.log, stderr.log} Note: By default, Bro will write ASCII logs to the data path above AND write JSON directly to Kafka. In general, you will be accessing the Bro data from Elasticsearch via Kibana .","title":"Bro"},{"location":"services/bro/#bro","text":"","title":"Bro"},{"location":"services/bro/#overview","text":"Bro is used to provide network protocol analysis within ROCK. It is extremely customizable, and it is encouraged that you take advantage of this. When deploying custom Bro scripts, please be sure to store them under a subdirectory of /usr/share/bro/site/scripts/ . We can't guarantee that your customizations won't be overwritten by Ansible if you don't follow this pattern.","title":"Overview"},{"location":"services/bro/#management","text":"","title":"Management"},{"location":"services/bro/#service","text":"Bro is deployed as a systemd unit, called bro . Normal systemd procedures apply here: sudo systemctl start bro sudo systemctl status bro sudo systemctl stop bro sudo systemctl restart bro The broctl command is now an alias. Using this alias prevents dangerous permission changes caused by running the real broctl binary with sudo. The only safe way otherwise to run broctl is to execute it as the bro user and bro group as such: `sudo -u bro -g bro /usr/bin/broctl`","title":"Service"},{"location":"services/bro/#directories","text":"Home /usr/share/bro/ Data /data/bro/logs/current/{stream_name.log} Application Logs /data/bro/logs/current/{stdout.log, stderr.log} Note: By default, Bro will write ASCII logs to the data path above AND write JSON directly to Kafka. In general, you will be accessing the Bro data from Elasticsearch via Kibana .","title":"Directories"},{"location":"services/docket/","text":"Docket \u00b6 Docket is a web UI that makes it easy for analysts to filter mountains of PCAP down to specific chunks in order to find the baddies . https:// /app/docket/ Overview \u00b6 PCAP is great, but doesn't scale well. There's so much detail that it can be overwhelming to sort through. A great alternate to \"following the TCP stream\" through an ocean of packets is to use a tool like Docket that allows for easy filtering on key points such as: timeframe hosts networks ports more ... The NSM community has needed a solution like Docket for a while, and we're excited to see how it empowers the analysis process. Basic Usage \u00b6 To access Docket point to https://<sensorip>/app/docket/ . Please note the trailing slash . (This is due to Kibana being served from the same proxy and gets greedy with the URL path). Submit Request \u00b6 Once into the UI simply add your search criteria and click \"Submit\". Get PCAP \u00b6 Once the job is processed, click \"Get PCAP\" to download to your box locally. Management \u00b6 Services \u00b6 Docket requires the following services to function: lighttpd - TLS connection stenographer - tool to write / query pcap stenographer@<int> - process for each monitor interface Current status can be checked with the following commands: sudo systemctl status lighttpd sudo rockctl status Directories \u00b6 Here are some important filesystem paths that will be useful for any necessary troubleshooting efforts: PCAP Storage \u00b6 User requested PCAP jobs are saved in: /var/spool/docket In a multi-user environment this directory can fill up depending on how much space has been allocated to the /var partition. Keep this path clean to prevent issues. Python Socket \u00b6 /run/docket/","title":"Docket"},{"location":"services/docket/#docket","text":"Docket is a web UI that makes it easy for analysts to filter mountains of PCAP down to specific chunks in order to find the baddies . https:// /app/docket/","title":"Docket"},{"location":"services/docket/#overview","text":"PCAP is great, but doesn't scale well. There's so much detail that it can be overwhelming to sort through. A great alternate to \"following the TCP stream\" through an ocean of packets is to use a tool like Docket that allows for easy filtering on key points such as: timeframe hosts networks ports more ... The NSM community has needed a solution like Docket for a while, and we're excited to see how it empowers the analysis process.","title":"Overview"},{"location":"services/docket/#basic-usage","text":"To access Docket point to https://<sensorip>/app/docket/ . Please note the trailing slash . (This is due to Kibana being served from the same proxy and gets greedy with the URL path).","title":"Basic Usage"},{"location":"services/docket/#submit-request","text":"Once into the UI simply add your search criteria and click \"Submit\".","title":"Submit Request"},{"location":"services/docket/#get-pcap","text":"Once the job is processed, click \"Get PCAP\" to download to your box locally.","title":"Get PCAP"},{"location":"services/docket/#management","text":"","title":"Management"},{"location":"services/docket/#services","text":"Docket requires the following services to function: lighttpd - TLS connection stenographer - tool to write / query pcap stenographer@<int> - process for each monitor interface Current status can be checked with the following commands: sudo systemctl status lighttpd sudo rockctl status","title":"Services"},{"location":"services/docket/#directories","text":"Here are some important filesystem paths that will be useful for any necessary troubleshooting efforts:","title":"Directories"},{"location":"services/docket/#pcap-storage","text":"User requested PCAP jobs are saved in: /var/spool/docket In a multi-user environment this directory can fill up depending on how much space has been allocated to the /var partition. Keep this path clean to prevent issues.","title":"PCAP Storage"},{"location":"services/docket/#python-socket","text":"/run/docket/","title":"Python Socket"},{"location":"services/elasticsearch/","text":"Elasticsearch \u00b6 Overview \u00b6 Elasticsearch is the data storage and retrieval system in RockNSM. Elasticsearch is an \"indexed JSON document store\". Unlike other solutions, (network) events are indexed once on initial ingest, and after which you can run queries and aggregations quickly and efficiently. ROCK sends all logs preformatted in JSON, complete with human readable timestamps. This does two things: Elasticsearch compression is effctively increased since there is not two copies of the data, raw and JSON. The preformatted timestamps and JSON log data greatly increase the logging and error rate while increasing reliability of the logging infrastructure. Management \u00b6 Service \u00b6 Elasticsearch is deployed as a systemd unit, called elasticsearch . Normal systemd procedures apply here: sudo systemctl start elasticsearch sudo systemctl status elasticsearch sudo systemctl stop elasticsearch sudo systemctl restart elasticsearch API Access \u00b6 Elasticsearch data can be accessed via a Restful API over port 9200. Kibana is the most common way this is done, but this can also be accomplished with curl commands, such as: $ curl <sensorip>:9200/_cat/indices . Directories \u00b6 home: /usr/share/elasticsearch data: /data/elasticsearch/ application logs: /var/log/elasticsearch/","title":"Elasticsearch"},{"location":"services/elasticsearch/#elasticsearch","text":"","title":"Elasticsearch"},{"location":"services/elasticsearch/#overview","text":"Elasticsearch is the data storage and retrieval system in RockNSM. Elasticsearch is an \"indexed JSON document store\". Unlike other solutions, (network) events are indexed once on initial ingest, and after which you can run queries and aggregations quickly and efficiently. ROCK sends all logs preformatted in JSON, complete with human readable timestamps. This does two things: Elasticsearch compression is effctively increased since there is not two copies of the data, raw and JSON. The preformatted timestamps and JSON log data greatly increase the logging and error rate while increasing reliability of the logging infrastructure.","title":"Overview"},{"location":"services/elasticsearch/#management","text":"","title":"Management"},{"location":"services/elasticsearch/#service","text":"Elasticsearch is deployed as a systemd unit, called elasticsearch . Normal systemd procedures apply here: sudo systemctl start elasticsearch sudo systemctl status elasticsearch sudo systemctl stop elasticsearch sudo systemctl restart elasticsearch","title":"Service"},{"location":"services/elasticsearch/#api-access","text":"Elasticsearch data can be accessed via a Restful API over port 9200. Kibana is the most common way this is done, but this can also be accomplished with curl commands, such as: $ curl <sensorip>:9200/_cat/indices .","title":"API Access"},{"location":"services/elasticsearch/#directories","text":"home: /usr/share/elasticsearch data: /data/elasticsearch/ application logs: /var/log/elasticsearch/","title":"Directories"},{"location":"services/filebeat/","text":"Filebeat \u00b6 Overview \u00b6 Elastic Beats are lightweight \"data shippers\". Filebeat's role in ROCK is to do just this: ship file data to the next step in the pipeline. The following ROCK components depend on Filebeat to send their log files into the Kafka message queue: Suricata - writes alerting data into eve.json FSF - writes static file scan results to rockout.log Management \u00b6 Service \u00b6 The filebeat service is configured and enabled on startup. This can be verified with either: $ sudo rockctl status $ sudo systemctl status filebeat Directories \u00b6 The configuration path for Filebeat is found at: /etc/filebeat/filebeat.yml","title":"Filebeat"},{"location":"services/filebeat/#filebeat","text":"","title":"Filebeat"},{"location":"services/filebeat/#overview","text":"Elastic Beats are lightweight \"data shippers\". Filebeat's role in ROCK is to do just this: ship file data to the next step in the pipeline. The following ROCK components depend on Filebeat to send their log files into the Kafka message queue: Suricata - writes alerting data into eve.json FSF - writes static file scan results to rockout.log","title":"Overview"},{"location":"services/filebeat/#management","text":"","title":"Management"},{"location":"services/filebeat/#service","text":"The filebeat service is configured and enabled on startup. This can be verified with either: $ sudo rockctl status $ sudo systemctl status filebeat","title":"Service"},{"location":"services/filebeat/#directories","text":"The configuration path for Filebeat is found at: /etc/filebeat/filebeat.yml","title":"Directories"},{"location":"services/fsf/","text":"FSF \u00b6 FSF is included in RockNSM to provide static file analysis on filetypes of interest. Overview \u00b6 FSF works in conjuction with the file extraction framework provided by Bro . Bro can be configured to watch for specific file (mime) types, as well as establishing max file sizes that will be extracted. FSF uses a client-server model and can watch for new extracted files in the /data/fsf/ partition. Management \u00b6 Services \u00b6 FSF is deployed as a systemd unit, called fsf . Normal systemd procedures apply here: sudo systemctl start fsf sudo systemctl status fsf sudo systemctl stop fsf sudo systemctl restart fsf It can also be managed using the rockctl command. Directories \u00b6 Server \u00b6 /opt/fsf/fsf-server/conf/config.py - main config file /opt/fsf/fsf-server/main.py - server script Client \u00b6 /opt/fsf/fsf-client/conf/config.py - main config file /opt/fsf/fsf-client/fsf_client.py - client binary","title":"FSF"},{"location":"services/fsf/#fsf","text":"FSF is included in RockNSM to provide static file analysis on filetypes of interest.","title":"FSF"},{"location":"services/fsf/#overview","text":"FSF works in conjuction with the file extraction framework provided by Bro . Bro can be configured to watch for specific file (mime) types, as well as establishing max file sizes that will be extracted. FSF uses a client-server model and can watch for new extracted files in the /data/fsf/ partition.","title":"Overview"},{"location":"services/fsf/#management","text":"","title":"Management"},{"location":"services/fsf/#services","text":"FSF is deployed as a systemd unit, called fsf . Normal systemd procedures apply here: sudo systemctl start fsf sudo systemctl status fsf sudo systemctl stop fsf sudo systemctl restart fsf It can also be managed using the rockctl command.","title":"Services"},{"location":"services/fsf/#directories","text":"","title":"Directories"},{"location":"services/fsf/#server","text":"/opt/fsf/fsf-server/conf/config.py - main config file /opt/fsf/fsf-server/main.py - server script","title":"Server"},{"location":"services/fsf/#client","text":"/opt/fsf/fsf-client/conf/config.py - main config file /opt/fsf/fsf-client/fsf_client.py - client binary","title":"Client"},{"location":"services/kafka/","text":"Kafka \u00b6 Kafka is a wicked fast and reliable message queue. Overview \u00b6 Kafka solves the problem of having multiple data sources sending into the same pipeline. It acts as a staging area to allow Logstash to keep up with things. Management \u00b6 Services \u00b6 Kafka is deployed as a systemd unit, called kafka . Normal systemd procedures apply here: sudo systemctl start kafka sudo systemctl status kafka sudo systemctl stop kafka sudo systemctl restart kafka It can also be managed using the rockctl command. Directories \u00b6 etc/kafka/server.properties - primary config file","title":"Kafka"},{"location":"services/kafka/#kafka","text":"Kafka is a wicked fast and reliable message queue.","title":"Kafka"},{"location":"services/kafka/#overview","text":"Kafka solves the problem of having multiple data sources sending into the same pipeline. It acts as a staging area to allow Logstash to keep up with things.","title":"Overview"},{"location":"services/kafka/#management","text":"","title":"Management"},{"location":"services/kafka/#services","text":"Kafka is deployed as a systemd unit, called kafka . Normal systemd procedures apply here: sudo systemctl start kafka sudo systemctl status kafka sudo systemctl stop kafka sudo systemctl restart kafka It can also be managed using the rockctl command.","title":"Services"},{"location":"services/kafka/#directories","text":"etc/kafka/server.properties - primary config file","title":"Directories"},{"location":"services/kibana/","text":"Kibana \u00b6 Overview \u00b6 Kibana is the web interface used to interact with data inside Elasticseach . Basic Usage \u00b6 Open a web browser and visit the following url: https://<sensorip>/app/kibana . On first connection, users will be prompted for a username and password. Upon running the deploy script a random passphrase is generated in the style of XKCD . These credentials are stored in \"KIBANA_CREDS.README\" file located in the home directory of the user created at install e.g. /home/admin/KIBANA_CREDS.README . Management \u00b6 Service \u00b6 Kibana is deployed as a systemd unit, called kibana . Normal systemd procedures apply here: sudo systemctl start kibana sudo systemctl status kibana sudo systemctl stop kibana sudo systemctl restart kibana Directories \u00b6 Home: /usr/share/kibana Data: Stored in .kibana index in Elasticseach Application Logs: journalctl -u kibana","title":"Kibana"},{"location":"services/kibana/#kibana","text":"","title":"Kibana"},{"location":"services/kibana/#overview","text":"Kibana is the web interface used to interact with data inside Elasticseach .","title":"Overview"},{"location":"services/kibana/#basic-usage","text":"Open a web browser and visit the following url: https://<sensorip>/app/kibana . On first connection, users will be prompted for a username and password. Upon running the deploy script a random passphrase is generated in the style of XKCD . These credentials are stored in \"KIBANA_CREDS.README\" file located in the home directory of the user created at install e.g. /home/admin/KIBANA_CREDS.README .","title":"Basic Usage"},{"location":"services/kibana/#management","text":"","title":"Management"},{"location":"services/kibana/#service","text":"Kibana is deployed as a systemd unit, called kibana . Normal systemd procedures apply here: sudo systemctl start kibana sudo systemctl status kibana sudo systemctl stop kibana sudo systemctl restart kibana","title":"Service"},{"location":"services/kibana/#directories","text":"Home: /usr/share/kibana Data: Stored in .kibana index in Elasticseach Application Logs: journalctl -u kibana","title":"Directories"},{"location":"services/logstash/","text":"Logstash \u00b6 Logstash is part of the Elastic Stack that performs log file filtering and enrichment. Management \u00b6 Services \u00b6 Logstash is deployed as a systemd unit, called logstash . Normal systemd procedures apply here: sudo systemctl start logstash sudo systemctl status logstash sudo systemctl stop logstash sudo systemctl restart logstash It can also be managed using the rockctl command. Directories \u00b6 /etc/logstash/ - main config path /etc/logstash/conf.d - ROCK specific config /var/lib/logstash - data path","title":"Logstash"},{"location":"services/logstash/#logstash","text":"Logstash is part of the Elastic Stack that performs log file filtering and enrichment.","title":"Logstash"},{"location":"services/logstash/#management","text":"","title":"Management"},{"location":"services/logstash/#services","text":"Logstash is deployed as a systemd unit, called logstash . Normal systemd procedures apply here: sudo systemctl start logstash sudo systemctl status logstash sudo systemctl stop logstash sudo systemctl restart logstash It can also be managed using the rockctl command.","title":"Services"},{"location":"services/logstash/#directories","text":"/etc/logstash/ - main config path /etc/logstash/conf.d - ROCK specific config /var/lib/logstash - data path","title":"Directories"},{"location":"services/stenographer/","text":"Stenographer \u00b6 ROCK uses Stenographer for full packet capture. Among other features, it provides the following advantages over other solutions: it's fast manages disk space will fill it's disk to 90% then start to overwrite oldest data forward Management \u00b6 Systemd \u00b6 Stenographer is deployed as a systemd unit, called stenographer . Normal systemd procedures apply here: sudo systemctl start stenographer sudo systemctl status stenographer sudo systemctl stop stenographer sudo systemctl restart stenographer Rockctl \u00b6 It can also be managed using the rockctl command. Multiple Interfaces \u00b6 It's important to note that Stenographer will have a (main) parent process, and a child process for every interface that it uses to capture packets. ex: STENOGRAPHER: Active: active ( exited ) since Mon 2019 -01-28 22 :51:47 UTC ; 1 weeks 0 days ago STENOGRAPHER@EM1: Active: active ( running ) since Mon 2019 -01-28 22 :51:47 UTC ; 1 weeks 0 days ago In order to restart all Stenographer processes, a wildcard can be used: sudo sytemctl restart stenographer* Directories \u00b6 Stenographer is great at managing it's own disk space, but that doesn't cut it when it's sharing the same mount point as Bro, Suricata , and other tools that generate data in ROCK. Best practice would be to create a /data/stenographer partition in order to prevent limited operations.","title":"Stenographer"},{"location":"services/stenographer/#stenographer","text":"ROCK uses Stenographer for full packet capture. Among other features, it provides the following advantages over other solutions: it's fast manages disk space will fill it's disk to 90% then start to overwrite oldest data forward","title":"Stenographer"},{"location":"services/stenographer/#management","text":"","title":"Management"},{"location":"services/stenographer/#systemd","text":"Stenographer is deployed as a systemd unit, called stenographer . Normal systemd procedures apply here: sudo systemctl start stenographer sudo systemctl status stenographer sudo systemctl stop stenographer sudo systemctl restart stenographer","title":"Systemd"},{"location":"services/stenographer/#rockctl","text":"It can also be managed using the rockctl command.","title":"Rockctl"},{"location":"services/stenographer/#multiple-interfaces","text":"It's important to note that Stenographer will have a (main) parent process, and a child process for every interface that it uses to capture packets. ex: STENOGRAPHER: Active: active ( exited ) since Mon 2019 -01-28 22 :51:47 UTC ; 1 weeks 0 days ago STENOGRAPHER@EM1: Active: active ( running ) since Mon 2019 -01-28 22 :51:47 UTC ; 1 weeks 0 days ago In order to restart all Stenographer processes, a wildcard can be used: sudo sytemctl restart stenographer*","title":"Multiple Interfaces"},{"location":"services/stenographer/#directories","text":"Stenographer is great at managing it's own disk space, but that doesn't cut it when it's sharing the same mount point as Bro, Suricata , and other tools that generate data in ROCK. Best practice would be to create a /data/stenographer partition in order to prevent limited operations.","title":"Directories"},{"location":"services/suricata/","text":"Suricata \u00b6 Suricata is IDS / Alerting solution in RockNSM. Overview \u00b6 Intrusion Detection Systems (IDS) are a great way to quickly alert on known bad. Alerts are triggered when a packet matches a defined pattern or signature . Management \u00b6 The Suricata service is configured and enabled on startup. Updating Rules \u00b6 The newest versions of Suricata come with the suricata-update command to manange and update rulesets. The official documentation is found here . Service \u00b6 Suricata is deployed as a systemd unit, called suricata . Normal systemd procedures apply here: sudo systemctl start suricata sudo systemctl status suricata sudo systemctl stop suricata sudo systemctl restart suricata It can also be managed using the rockctl command. Directories \u00b6 /etc/suricata/ - configuration path","title":"Suricata"},{"location":"services/suricata/#suricata","text":"Suricata is IDS / Alerting solution in RockNSM.","title":"Suricata"},{"location":"services/suricata/#overview","text":"Intrusion Detection Systems (IDS) are a great way to quickly alert on known bad. Alerts are triggered when a packet matches a defined pattern or signature .","title":"Overview"},{"location":"services/suricata/#management","text":"The Suricata service is configured and enabled on startup.","title":"Management"},{"location":"services/suricata/#updating-rules","text":"The newest versions of Suricata come with the suricata-update command to manange and update rulesets. The official documentation is found here .","title":"Updating Rules"},{"location":"services/suricata/#service","text":"Suricata is deployed as a systemd unit, called suricata . Normal systemd procedures apply here: sudo systemctl start suricata sudo systemctl status suricata sudo systemctl stop suricata sudo systemctl restart suricata It can also be managed using the rockctl command.","title":"Service"},{"location":"services/suricata/#directories","text":"/etc/suricata/ - configuration path","title":"Directories"},{"location":"usage/","text":"Basic Usage \u00b6 Key Interfaces \u00b6 Kibana - https://localhost \u00b6 The generated credentials are in the home directory of the user created at install: ~/KIBANA_CREDS.README Docket - https://localhost:8443 \u00b6 Docket - web interface for pulling PCAP from the sensor (must be enabled in config) localhost or IP of the management interface of the box Functions Checks \u00b6 After the initial build, the ES cluster will be yellow because the marvel index will think it's missing a replica. Run this to fix this issue. This job will run from cron just after midnight every day: `/usr/local/bin/es_cleanup.sh 2&gt;&amp;1 &gt; /dev/null` Check to see that the ES cluster says it's green: `curl -s localhost:9200/_cluster/health | jq '.'` See how many documents are in the indexes. The count should be non-zero: `curl -s localhost:9200/_all/_count | jq '.'` You can fire some traffic across the sensor at this point to see if it's collecting. NOTE: This requires that you upload your own test PCAP to the box. `sudo tcpreplay -i [your monitor interface] /path/to/a/test.pcap` After replaying some traffic, or just waiting a bit, the count should be going up. You should have plain text bro logs showing up in /data/bro/logs/current/: `ls -ltr /data/bro/logs/current/` Rockctl \u00b6 These functions are accomplished with: sudo rock_status - get the status of ROCK services sudo rock_start - start ROCK services sudo rock_stop - stop ROCK services","title":"Basic Operation"},{"location":"usage/#basic-usage","text":"","title":"Basic Usage"},{"location":"usage/#key-interfaces","text":"","title":"Key Interfaces"},{"location":"usage/#kibana-httpslocalhost","text":"The generated credentials are in the home directory of the user created at install: ~/KIBANA_CREDS.README","title":"Kibana - https://localhost"},{"location":"usage/#docket-httpslocalhost8443","text":"Docket - web interface for pulling PCAP from the sensor (must be enabled in config) localhost or IP of the management interface of the box","title":"Docket - https://localhost:8443"},{"location":"usage/#functions-checks","text":"After the initial build, the ES cluster will be yellow because the marvel index will think it's missing a replica. Run this to fix this issue. This job will run from cron just after midnight every day: `/usr/local/bin/es_cleanup.sh 2&gt;&amp;1 &gt; /dev/null` Check to see that the ES cluster says it's green: `curl -s localhost:9200/_cluster/health | jq '.'` See how many documents are in the indexes. The count should be non-zero: `curl -s localhost:9200/_all/_count | jq '.'` You can fire some traffic across the sensor at this point to see if it's collecting. NOTE: This requires that you upload your own test PCAP to the box. `sudo tcpreplay -i [your monitor interface] /path/to/a/test.pcap` After replaying some traffic, or just waiting a bit, the count should be going up. You should have plain text bro logs showing up in /data/bro/logs/current/: `ls -ltr /data/bro/logs/current/`","title":"Functions Checks"},{"location":"usage/#rockctl","text":"These functions are accomplished with: sudo rock_status - get the status of ROCK services sudo rock_start - start ROCK services sudo rock_stop - stop ROCK services","title":"Rockctl"},{"location":"usage/support/","text":"Support Guide \u00b6 This section aims to smooth out the most frequent issues new users will run into. Autodetect Assumptions \u00b6 When writing the scripts to generate default values, we had to make some assumptions. The defaults are generated according to these assumptions and should generally work if your sensor aligns with them. That said, these assumptions will give you a working sensor, but may need some love for higher performance. If you cannot meet these assumptions, look at the indicated configuration variables in /etc/rocknsm/config.yml for workaround approaches (with impact on performance). Two Network Interfaces: a management interface with a default route an interface without a default route (defined by rock_monifs ) TIP: We assume that any interface that does not have a default route will be used for collection. Each sensor application will be configured accordingly. WARNING : This so far has been the number one problem with a fresh install for beta testers!! Check your interface configuration!! You have mounted your largest storage volume(s) under /data/ (defined by rock_data_dir ) Your hostname (FQDN) is defined in the playbooks/inventory/all-in-one.ini file You allow management via SSH from any network (defined by rock_mgmt_nets ) You wish to use Bro, Suricata, Stenographer (disabled by default) and the whole data pipeline. (See with_* options) If installed via ISO, you will perform an offline install, else we assume online (defined by rock_online_install ) Bro will use half of your CPU resources, up to 8 CPUs We will continue to add more support information as the userbase grows. Deployment Script \u00b6 If you find the deployment is failing, the script can be run with very verbose output. This example will write the output to a file for review: `DEBUG=1 ./deploy_rock.sh | tee /tmp/deploy_rock.log` Log Timestamps \u00b6 UTC is generally preferred for logging data as the timestamps from anywhere in the world will have a proper order without calculating offsets. That said, Kibana will present the bro logs according to your timezone (as set in the browser). The bro logs themselves (i.e. in /data/bro/logs/) log in epoch time and will be written in UTC regardless of the system timezone. Bro includes a utility for parsing these on the command line called bro-cut . It can be used to print human-readable timestamps in either the local sensor timezone or UTC. You can also give it a custom format string to specify what you'd like displayed.","title":"Support"},{"location":"usage/support/#support-guide","text":"This section aims to smooth out the most frequent issues new users will run into.","title":"Support Guide"},{"location":"usage/support/#autodetect-assumptions","text":"When writing the scripts to generate default values, we had to make some assumptions. The defaults are generated according to these assumptions and should generally work if your sensor aligns with them. That said, these assumptions will give you a working sensor, but may need some love for higher performance. If you cannot meet these assumptions, look at the indicated configuration variables in /etc/rocknsm/config.yml for workaround approaches (with impact on performance). Two Network Interfaces: a management interface with a default route an interface without a default route (defined by rock_monifs ) TIP: We assume that any interface that does not have a default route will be used for collection. Each sensor application will be configured accordingly. WARNING : This so far has been the number one problem with a fresh install for beta testers!! Check your interface configuration!! You have mounted your largest storage volume(s) under /data/ (defined by rock_data_dir ) Your hostname (FQDN) is defined in the playbooks/inventory/all-in-one.ini file You allow management via SSH from any network (defined by rock_mgmt_nets ) You wish to use Bro, Suricata, Stenographer (disabled by default) and the whole data pipeline. (See with_* options) If installed via ISO, you will perform an offline install, else we assume online (defined by rock_online_install ) Bro will use half of your CPU resources, up to 8 CPUs We will continue to add more support information as the userbase grows.","title":"Autodetect Assumptions"},{"location":"usage/support/#deployment-script","text":"If you find the deployment is failing, the script can be run with very verbose output. This example will write the output to a file for review: `DEBUG=1 ./deploy_rock.sh | tee /tmp/deploy_rock.log`","title":"Deployment Script"},{"location":"usage/support/#log-timestamps","text":"UTC is generally preferred for logging data as the timestamps from anywhere in the world will have a proper order without calculating offsets. That said, Kibana will present the bro logs according to your timezone (as set in the browser). The bro logs themselves (i.e. in /data/bro/logs/) log in epoch time and will be written in UTC regardless of the system timezone. Bro includes a utility for parsing these on the command line called bro-cut . It can be used to print human-readable timestamps in either the local sensor timezone or UTC. You can also give it a custom format string to specify what you'd like displayed.","title":"Log Timestamps"}]}